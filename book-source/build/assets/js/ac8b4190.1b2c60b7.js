"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[755],{8052:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Autonomous-Navigation/visual-slam-systems","title":"Lesson 2: Visual SLAM Systems","description":"Duration CEFR B1-C2 | Priority Lesson 1","source":"@site/docs/04-Autonomous-Navigation/02-visual-slam-systems.md","sourceDirName":"04-Autonomous-Navigation","slug":"/Autonomous-Navigation/visual-slam-systems","permalink":"/docs/Autonomous-Navigation/visual-slam-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/ai-book/tree/main/docs/04-Autonomous-Navigation/02-visual-slam-systems.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1: Navigation and Localization Overview","permalink":"/docs/Autonomous-Navigation/navigation-and-localization-overview"},"next":{"title":"Lesson 3: Introduction to Isaac Sim","permalink":"/docs/Autonomous-Navigation/introduction-to-isaac-sim"}}');var i=r(4848),t=r(8453);const a={},o="Lesson 2: Visual SLAM Systems",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Layer 1: Foundation",id:"layer-1-foundation",level:2},{value:"2.1 ORB-SLAM3 Architecture",id:"21-orb-slam3-architecture",level:3},{value:"2.2 Feature Detection Pipeline",id:"22-feature-detection-pipeline",level:3},{value:"2.3 Feature Matching",id:"23-feature-matching",level:3},{value:"2.4 Pose Estimation from Feature Matches",id:"24-pose-estimation-from-feature-matches",level:3},{value:"2.5 Local Mapping",id:"25-local-mapping",level:3},{value:"Layer 2: Collaboration (ROS 2 Integration)",id:"layer-2-collaboration-ros-2-integration",level:2},{value:"2.1 ORB-SLAM3 ROS 2 Wrapper",id:"21-orb-slam3-ros-2-wrapper",level:3},{value:"2.2 RViz Visualization",id:"22-rviz-visualization",level:3},{value:"Layer 3: Intelligence (Configuration &amp; Tuning)",id:"layer-3-intelligence-configuration--tuning",level:2},{value:"3.1 Camera Calibration",id:"31-camera-calibration",level:3},{value:"3.2 Monocular VSLAM Configuration",id:"32-monocular-vslam-configuration",level:3},{value:"3.3 Stereo VSLAM Configuration",id:"33-stereo-vslam-configuration",level:3},{value:"3.4 Performance Optimization",id:"34-performance-optimization",level:3},{value:"Layer 4: Advanced",id:"layer-4-advanced",level:2},{value:"4.1 Loop Closure Detection",id:"41-loop-closure-detection",level:3},{value:"4.2 Inertial Measurement Unit (IMU) Integration",id:"42-inertial-measurement-unit-imu-integration",level:3},{value:"Summary",id:"summary",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Example 1: Configure ORB-SLAM3 for Gazebo",id:"example-1-configure-orb-slam3-for-gazebo",level:3},{value:"Example 2: Analyze Loop Closure Performance",id:"example-2-analyze-loop-closure-performance",level:3},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"References",id:"references",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-2-visual-slam-systems",children:"Lesson 2: Visual SLAM Systems"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Duration"}),": 3 hours | ",(0,i.jsx)(n.strong,{children:"Level"}),": CEFR B1-C2 | ",(0,i.jsx)(n.strong,{children:"Priority"}),": P1 | ",(0,i.jsx)(n.strong,{children:"Prerequisite"}),": Lesson 1"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understand"})," ORB-SLAM3 architecture and feature-based visual SLAM"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explain"})," feature detection, matching, and pose estimation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configure"})," ORB-SLAM3 for monocular and stereo camera setups"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Run"})," VSLAM on simulated Gazebo data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Measure"})," and optimize odometry accuracy"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Debug"})," SLAM failures (insufficient features, lighting issues)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"layer-1-foundation",children:"Layer 1: Foundation"}),"\n",(0,i.jsx)(n.h3,{id:"21-orb-slam3-architecture",children:"2.1 ORB-SLAM3 Architecture"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ORB"})," = ",(0,i.jsx)(n.strong,{children:"O"}),"riented ",(0,i.jsx)(n.strong,{children:"R"}),"FAST and ",(0,i.jsx)(n.strong,{children:"B"}),"RIEF"]}),"\n",(0,i.jsx)(n.p,{children:"ORB-SLAM3 is a complete visual SLAM system using ORB features. Why ORB?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fast Detection"}),": FAST corner detection (~1000 features/frame)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rotation-Invariant"}),": BRIEF descriptors handle camera rotation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Efficient"}),": Low computational cost, real-time performance"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Complete system"})," includes:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tracking"}),": Feature detection and matching frame-to-frame"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mapping"}),": Accumulating features into global structure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop Closing"}),": Detecting and correcting revisited locations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bundle Adjustment"}),": Global optimization to minimize error"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"22-feature-detection-pipeline",children:"2.2 Feature Detection Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Raw image\r\n    \u2502\r\n    \u251c\u2500\u2192 FAST Corner Detection\r\n    \u2502       (identify key points)\r\n    \u2502\r\n    \u251c\u2500\u2192 BRIEF Descriptor Computation\r\n    \u2502       (binary feature description)\r\n    \u2502\r\n    \u2514\u2500\u2192 Feature List\r\n            [pt1, pt2, ..., ptN]\r\n            (locations + descriptors)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"FAST (Features from Accelerated Segment Test)"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Detect corners by comparing pixel intensities around candidate point"}),"\n",(0,i.jsx)(n.li,{children:"Fast because it uses simple intensity comparisons"}),"\n",(0,i.jsx)(n.li,{children:"~1000-2000 features per frame in textured scenes"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"BRIEF (Binary Robust Independent Elementary Features)"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Create binary descriptor (256 bits) for each feature"}),"\n",(0,i.jsx)(n.li,{children:"Fast to compute and match"}),"\n",(0,i.jsx)(n.li,{children:"Rotation variant (ORB variants fix this)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"23-feature-matching",children:"2.3 Feature Matching"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Frame t:  features [f1, f2, f3, ..., fn]\r\n               \u2502\r\n               \u251c\u2500\u2192 Descriptor matching with Frame t+1\r\n               \u2502\r\nFrame t+1: features [f'1, f'2, f'3, ..., f'm]\r\n\r\nResult: Matched pairs = [(f1, f'3), (f2, f'1), ...]\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Matching strategy"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Compare BRIEF descriptors using Hamming distance"}),"\n",(0,i.jsx)(n.li,{children:"For each feature in frame t, find closest match in frame t+1"}),"\n",(0,i.jsx)(n.li,{children:"Use bidirectional matching (must match both directions)"}),"\n",(0,i.jsx)(n.li,{children:"Use outlier rejection (RANSAC) to remove false matches"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"24-pose-estimation-from-feature-matches",children:"2.4 Pose Estimation from Feature Matches"}),"\n",(0,i.jsx)(n.p,{children:"Given matched features between consecutive frames, estimate camera motion:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"3D point P(world) \u2500\u2500projection\u2500\u2500\u2192 p1 (in frame t)\r\n                                  p2 (in frame t+1)\r\n\r\nFrom p1 \u2194 p2 correspondence, recover:\r\n- Rotation: R (3\xd73 matrix)\r\n- Translation: t (3D vector)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Method"}),": Essential matrix decomposition"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Essential matrix E encodes both R and t"}),"\n",(0,i.jsx)(n.li,{children:"Computed from matched feature correspondences"}),"\n",(0,i.jsx)(n.li,{children:"Decompose E to recover R and t"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"25-local-mapping",children:"2.5 Local Mapping"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Keyframes"}),': Not every frame is stored; only "keyframes" with sufficient motion/features']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Frame sequence:\r\n[Keyframe1] [frame2] [frame3] [Keyframe4] [frame5]\r\n    (stored)  (used)   (used)    (stored)   (used)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why keyframes?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Reduces memory usage"}),"\n",(0,i.jsx)(n.li,{children:"Speeds up processing"}),"\n",(0,i.jsx)(n.li,{children:"Filters out static frames"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Local mapping process"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"New keyframe arrives"}),"\n",(0,i.jsx)(n.li,{children:"Add new 3D points from features without depth"}),"\n",(0,i.jsx)(n.li,{children:"Perform local bundle adjustment (optimize pose + structure)"}),"\n",(0,i.jsx)(n.li,{children:"Culling: Remove redundant points/keyframes"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"layer-2-collaboration-ros-2-integration",children:"Layer 2: Collaboration (ROS 2 Integration)"}),"\n",(0,i.jsx)(n.h3,{id:"21-orb-slam3-ros-2-wrapper",children:"2.1 ORB-SLAM3 ROS 2 Wrapper"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Camera calibration parameters"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# camera_calibration.yaml\r\nCamera.fx: 500.0  # focal length x (pixels)\r\nCamera.fy: 500.0  # focal length y (pixels)\r\nCamera.cx: 320.0  # principal point x (pixels)\r\nCamera.cy: 240.0  # principal point y (pixels)\r\nCamera.k1: 0.0    # distortion coefficient\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Camera topic interface"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ROS 2 Topic: /camera/image_raw\r\n  \u251c\u2500 Type: sensor_msgs/msg/Image\r\n  \u251c\u2500 Encoding: rgb8 or mono8\r\n  \u2514\u2500 Frequency: 15-30 Hz (typical)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ORB-SLAM3 output topics"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'/orb_slam3/pose           (nav_msgs/Odometry)\r\n  \u251c\u2500 pose.pose (position + orientation)\r\n  \u251c\u2500 twist.twist (velocity estimate)\r\n  \u2514\u2500 header.frame_id = "map"\r\n\r\n/orb_slam3/trajectory     (geometry_msgs/PoseStamped array)\r\n  \u251c\u2500 All poses in trajectory\r\n  \u2514\u2500 For visualization/analysis\r\n\r\n/orb_slam3/tracked_points (sensor_msgs/PointCloud2)\r\n  \u251c\u2500 3D positions of tracked features\r\n  \u2514\u2500 Used in RViz visualization\n'})}),"\n",(0,i.jsx)(n.h3,{id:"22-rviz-visualization",children:"2.2 RViz Visualization"}),"\n",(0,i.jsx)(n.p,{children:"Monitor SLAM progress in real-time:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"RViz GUI:\r\n\u251c\u2500 Map frame (map)\r\n\u2502  \u251c\u2500 Tracked feature points (colored by depth)\r\n\u2502  \u251c\u2500 Estimated trajectory (green)\r\n\u2502  \u2514\u2500 Ground truth trajectory (red, if available)\r\n\u251c\u2500 Robot frame (base_link)\r\n\u2502  \u251c\u2500 Current pose\r\n\u2502  \u2514\u2500 Odometry frame\r\n\u2514\u2500 Camera view\r\n   \u251c\u2500 Detected features (circles)\r\n   \u2514\u2500 Tracked features (blue)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"layer-3-intelligence-configuration--tuning",children:"Layer 3: Intelligence (Configuration & Tuning)"}),"\n",(0,i.jsx)(n.h3,{id:"31-camera-calibration",children:"3.1 Camera Calibration"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Critical"}),": ORB-SLAM3 requires accurate camera calibration"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Calibration parameters"})," (from camera matrix K):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"K = [fx  0  cx]\r\n    [0  fy  cy]\r\n    [0   0   1]\r\n\r\nWhere:\r\n- fx, fy = focal length (pixels)\r\n- cx, cy = principal point (pixels)\r\n- Distortion coefficients: k1, k2, p1, p2, k3\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For Gazebo simulated cameras"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use ideal parameters (no distortion)"}),"\n",(0,i.jsx)(n.li,{children:"fx = fy = focal_length (from camera plugin)"}),"\n",(0,i.jsx)(n.li,{children:"cx = image_width / 2"}),"\n",(0,i.jsx)(n.li,{children:"cy = image_height / 2"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"32-monocular-vslam-configuration",children:"3.2 Monocular VSLAM Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# mono_settings.yaml\r\nCamera.type: "PinHole"\r\nCamera.width: 640\r\nCamera.height: 480\r\nCamera.fx: 500.0\r\nCamera.fy: 500.0\r\nCamera.cx: 320.0\r\nCamera.cy: 240.0\r\n\r\n# Feature detection\r\nORBextractor.nFeatures: 2000  # features per frame\r\nORBextractor.scaleFactor: 1.2 # image pyramid scale\r\nORBextractor.nLevels: 8       # pyramid levels\r\n\r\n# Thresholds\r\nThFAST: 20      # FAST corner threshold\r\nThFASTinit: 50  # initial frame threshold\r\n\r\n# ORB-SLAM3 specific\r\nSystem.useStereo: false\r\nSystem.useRGBD: false\r\nSystem.useIMU: true  # use IMU if available\n'})}),"\n",(0,i.jsx)(n.h3,{id:"33-stereo-vslam-configuration",children:"3.3 Stereo VSLAM Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# stereo_settings.yaml\r\nCamera.type: "PinHole"\r\nCamera.width: 640\r\nCamera.height: 480\r\n\r\n# Left camera (reference)\r\nCamera.fx: 500.0\r\nCamera.fy: 500.0\r\nCamera.cx: 320.0\r\nCamera.cy: 240.0\r\n\r\n# Stereo baseline and other parameters\r\nStereo.baseline: 0.1  # baseline between cameras (meters)\r\nStereo.ThDepth: 40    # close/far threshold (pixels)\r\n\r\n# Feature detection (same as mono)\r\nORBextractor.nFeatures: 2000\r\nORBextractor.scaleFactor: 1.2\r\n\r\nSystem.useStereo: true\n'})}),"\n",(0,i.jsx)(n.h3,{id:"34-performance-optimization",children:"3.4 Performance Optimization"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parameter tuning for different scenarios"}),":"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Scenario"}),(0,i.jsx)(n.th,{children:"nFeatures"}),(0,i.jsx)(n.th,{children:"scaleFactor"}),(0,i.jsx)(n.th,{children:"ThFAST"}),(0,i.jsx)(n.th,{children:"Goal"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Textured indoors"}),(0,i.jsx)(n.td,{children:"2000"}),(0,i.jsx)(n.td,{children:"1.2"}),(0,i.jsx)(n.td,{children:"20"}),(0,i.jsx)(n.td,{children:"Balance accuracy and speed"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Low texture"}),(0,i.jsx)(n.td,{children:"4000"}),(0,i.jsx)(n.td,{children:"1.2"}),(0,i.jsx)(n.td,{children:"10"}),(0,i.jsx)(n.td,{children:"More features, lenient detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"High speed"}),(0,i.jsx)(n.td,{children:"1000"}),(0,i.jsx)(n.td,{children:"1.3"}),(0,i.jsx)(n.td,{children:"30"}),(0,i.jsx)(n.td,{children:"Fast computation, sparse features"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"High accuracy"}),(0,i.jsx)(n.td,{children:"3000"}),(0,i.jsx)(n.td,{children:"1.2"}),(0,i.jsx)(n.td,{children:"15"}),(0,i.jsx)(n.td,{children:"More features, careful detection"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Computational cost"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Feature detection: ~30-50ms"}),"\n",(0,i.jsx)(n.li,{children:"Feature matching: ~20-40ms"}),"\n",(0,i.jsx)(n.li,{children:"Pose estimation: ~10-20ms"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total per frame"}),": 60-110ms @ 15Hz = reasonable"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"layer-4-advanced",children:"Layer 4: Advanced"}),"\n",(0,i.jsx)(n.h3,{id:"41-loop-closure-detection",children:"4.1 Loop Closure Detection"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": After 10 minutes of navigation, how does SLAM know if it has returned to a known location?"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Place recognition using visual similarity"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Loop closure pipeline:\r\n\u251c\u2500 Current image\r\n\u2502  \u251c\u2500 Extract features\r\n\u2502  \u2514\u2500 Compute feature vector ("bag of words")\r\n\u2502\r\n\u251c\u2500 Query database of past keyframe bags\r\n\u2502  \u2514\u2500 Find keyframes with similar bags\r\n\u2502\r\n\u251c\u2500 Candidate matches found?\r\n\u2502  \u2514\u2500 YES \u2192 Perform geometric verification\r\n\u2502       \u251c\u2500 Match features between current and candidate\r\n\u2502       \u251c\u2500 Compute similarity transform\r\n\u2502       \u2514\u2500 If match strong \u2192 declare loop closure\r\n\u2502\r\n\u2514\u2500 Loop closure correction:\r\n   \u251c\u2500 Optimize pose graph\r\n   \u251c\u2500 Correct all accumulated drift\r\n   \u2514\u2500 Fuse with previous estimate\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why geometric verification is crucial"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Perceptual aliasing: Different places look similar"}),"\n",(0,i.jsx)(n.li,{children:"Need to verify with feature matches, not just appearance"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"42-inertial-measurement-unit-imu-integration",children:"4.2 Inertial Measurement Unit (IMU) Integration"}),"\n",(0,i.jsx)(n.p,{children:"ORB-SLAM3 can fuse camera + IMU:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Benefits"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"IMU provides immediate angular velocity information"}),"\n",(0,i.jsx)(n.li,{children:"Helps when visual features insufficient (motion blur, low light)"}),"\n",(0,i.jsx)(n.li,{children:"Enables scale recovery in monocular mode"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Challenge"}),": Temporal synchronization"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera and IMU have different timestamp frequencies"}),"\n",(0,i.jsx)(n.li,{children:"Must align on common time reference"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Key Insight"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ORB features"})}),(0,i.jsx)(n.td,{children:"Fast to detect/match, rotation-invariant, enable real-time SLAM"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Keyframes"})}),(0,i.jsx)(n.td,{children:"Not every frame is stored; only keyframes reduce computation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Feature matching"})}),(0,i.jsx)(n.td,{children:"Descriptor matching + RANSAC outlier rejection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Pose estimation"})}),(0,i.jsx)(n.td,{children:"Essential matrix decomposition from matched features"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Loop closure"})}),(0,i.jsx)(n.td,{children:"Place recognition + geometric verification to correct drift"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"IMU fusion"})}),(0,i.jsx)(n.td,{children:"Improves robustness in challenging conditions"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-configure-orb-slam3-for-gazebo",children:"Example 1: Configure ORB-SLAM3 for Gazebo"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# code_examples/orb_slam3_config_generator.py\r\nimport yaml\r\n\r\ndef generate_mono_config():\r\n    \"\"\"Generate ORB-SLAM3 config for monocular Gazebo camera\"\"\"\r\n    config = {\r\n        'Camera': {\r\n            'type': 'PinHole',\r\n            'width': 640,\r\n            'height': 480,\r\n            'fx': 500.0,\r\n            'fy': 500.0,\r\n            'cx': 320.0,\r\n            'cy': 240.0,\r\n        },\r\n        'ORBextractor': {\r\n            'nFeatures': 2000,\r\n            'scaleFactor': 1.2,\r\n            'nLevels': 8,\r\n        },\r\n        'ThFAST': 20,\r\n        'System': {\r\n            'useStereo': False,\r\n            'useRGBD': False,\r\n            'useIMU': False,\r\n        }\r\n    }\r\n\r\n    with open('mono_settings.yaml', 'w') as f:\r\n        yaml.dump(config, f)\r\n    return config\r\n\r\nif __name__ == '__main__':\r\n    generate_mono_config()\r\n    print(\"Config saved to mono_settings.yaml\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-analyze-loop-closure-performance",children:"Example 2: Analyze Loop Closure Performance"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# code_examples/analyze_loop_closure.py\r\nimport rclpy\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport numpy as np\r\n\r\nclass LoopClosureAnalyzer(rclpy.Node):\r\n    def __init__(self):\r\n        super().__init__('loop_closure_analyzer')\r\n        self.poses = []\r\n        self.loop_closure_count = 0\r\n\r\n        # Subscribe to poses\r\n        self.create_subscription(PoseStamped, '/orb_slam3/trajectory',\r\n                                self.pose_callback, 10)\r\n        self.create_timer(5.0, self.analyze_loop_closures)\r\n\r\n    def pose_callback(self, msg):\r\n        pose = msg.pose.position\r\n        self.poses.append([pose.x, pose.y, pose.z])\r\n\r\n    def analyze_loop_closures(self):\r\n        \"\"\"Detect loop closures by analyzing trajectory\"\"\"\r\n        if len(self.poses) < 20:\r\n            return\r\n\r\n        poses_array = np.array(self.poses)\r\n        current = poses_array[-1]\r\n\r\n        # Find past poses within 0.5m distance\r\n        distances = np.linalg.norm(poses_array[:-20] - current, axis=1)\r\n        close_poses = np.where(distances < 0.5)[0]\r\n\r\n        if len(close_poses) > 0:\r\n            first_close = close_poses[0]\r\n            time_since = len(poses_array) - first_close\r\n            self.get_logger().info(\r\n                f'Potential loop closure detected! '\r\n                f'Revisited pose from {time_since} frames ago'\r\n            )\r\n            self.loop_closure_count += 1\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Configure ORB-SLAM3 and measure tracking performance"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Create Gazebo world with diverse features (walls, textures, objects)"}),"\n",(0,i.jsx)(n.li,{children:"Generate camera calibration config for your Gazebo camera"}),"\n",(0,i.jsxs)(n.li,{children:["Launch ORB-SLAM3: ",(0,i.jsx)(n.code,{children:"ros2 launch code_examples/orb_slam3_launch.py"})]}),"\n",(0,i.jsx)(n.li,{children:"Record SLAM output to ROS bag"}),"\n",(0,i.jsxs)(n.li,{children:["Analyze:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Number of tracked features per frame"}),"\n",(0,i.jsx)(n.li,{children:"Loop closure detections"}),"\n",(0,i.jsx)(n.li,{children:"Pose estimation accuracy"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Success criteria"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"500 features tracked continuously"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Smooth trajectory (no sudden jumps)"}),"\n",(0,i.jsx)(n.li,{children:"Loop closures detected when revisiting areas"}),"\n",(0,i.jsx)(n.li,{children:"ATE <5% of path length"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ORB-SLAM3 Paper"}),": ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2007.11898",children:"ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feature Detection"}),": ",(0,i.jsx)(n.a,{href:"https://www.edwardrosten.com/work/fast.html",children:"FAST Corner Detection"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"BRIEF Descriptors"}),": ",(0,i.jsx)(n.a,{href:"https://www.cs.ubc.ca/~lowe/papers/calonder_eccv10.pdf",children:"BRIEF: Computing a Local Binary Descriptor Very Fast"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"You now understand the architecture and tuning of ORB-SLAM3. Choose your next lesson:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["\u2192 ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/nav2-path-planning-stack",children:"Lesson 4: Nav2 Path Planning"})]})," (Core: use SLAM for navigation)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["\u2192 ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/multi-sensor-perception-and-fusion",children:"Lesson 7: Sensor Fusion"})]})," (Advanced: combine SLAM with LiDAR/IMU)"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Lesson 2 Summary"}),": ORB-SLAM3 provides complete visual SLAM using ORB features, keyframe tracking, and loop closure detection. Understanding feature detection, matching, pose estimation, and loop closure enables you to configure and debug VSLAM systems in diverse scenarios."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var s=r(6540);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);