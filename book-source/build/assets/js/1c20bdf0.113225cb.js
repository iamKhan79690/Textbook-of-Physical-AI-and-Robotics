"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[901],{4186:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Autonomous-Navigation/navigation-and-localization-overview","title":"Lesson 1: Navigation and Localization Overview","description":"Duration CEFR B1-C2 | Priority: P1","source":"@site/docs/04-Autonomous-Navigation/01-navigation-and-localization-overview.md","sourceDirName":"04-Autonomous-Navigation","slug":"/Autonomous-Navigation/navigation-and-localization-overview","permalink":"/docs/Autonomous-Navigation/navigation-and-localization-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/ai-book/tree/main/docs/04-Autonomous-Navigation/01-navigation-and-localization-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Autonomous Navigation","permalink":"/docs/Autonomous-Navigation/intro"},"next":{"title":"Lesson 2: Visual SLAM Systems","permalink":"/docs/Autonomous-Navigation/visual-slam-systems"}}');var i=s(4848),o=s(8453);const l={},t="Lesson 1: Navigation and Localization Overview",a={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Layer 1: Foundation",id:"layer-1-foundation",level:2},{value:"1.1 What is Autonomous Navigation?",id:"11-what-is-autonomous-navigation",level:3},{value:"1.2 The Three Hard Problems of Autonomous Navigation",id:"12-the-three-hard-problems-of-autonomous-navigation",level:3},{value:"Problem 1: Localization (Where am I?)",id:"problem-1-localization-where-am-i",level:4},{value:"Problem 2: Mapping (What&#39;s around me?)",id:"problem-2-mapping-whats-around-me",level:4},{value:"Problem 3: Planning (How do I get there?)",id:"problem-3-planning-how-do-i-get-there",level:4},{value:"1.3 SLAM: The Elegant Solution",id:"13-slam-the-elegant-solution",level:3},{value:"1.4 Visual Odometry vs. Full SLAM",id:"14-visual-odometry-vs-full-slam",level:3},{value:"Visual Odometry (VO)",id:"visual-odometry-vo",level:4},{value:"Visual SLAM (full SLAM)",id:"visual-slam-full-slam",level:4},{value:"1.5 Monocular vs. Stereo VSLAM",id:"15-monocular-vs-stereo-vslam",level:3},{value:"Monocular SLAM",id:"monocular-slam",level:4},{value:"Stereo SLAM",id:"stereo-slam",level:4},{value:"Layer 2: Collaboration (ROS 2 Integration)",id:"layer-2-collaboration-ros-2-integration",level:2},{value:"2.1 SLAM in ROS 2",id:"21-slam-in-ros-2",level:3},{value:"2.2 Key Topics and Services",id:"22-key-topics-and-services",level:3},{value:"2.3 Coordinate Frames",id:"23-coordinate-frames",level:3},{value:"Layer 3: Intelligence (Tuning and Optimization)",id:"layer-3-intelligence-tuning-and-optimization",level:2},{value:"3.1 Loop Closure Detection",id:"31-loop-closure-detection",level:3},{value:"3.2 Tuning SLAM Parameters",id:"32-tuning-slam-parameters",level:3},{value:"3.3 Accuracy Metrics",id:"33-accuracy-metrics",level:3},{value:"Layer 4: Advanced (Research Frontiers)",id:"layer-4-advanced-research-frontiers",level:2},{value:"4.1 Beyond ORB-SLAM3",id:"41-beyond-orb-slam3",level:3},{value:"4.2 Loop Closure in Real-World Systems",id:"42-loop-closure-in-real-world-systems",level:3},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment",id:"self-assessment",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Example 1: Launch ORB-SLAM3 on Gazebo",id:"example-1-launch-orb-slam3-on-gazebo",level:3},{value:"Example 2: Verify Odometry Accuracy",id:"example-2-verify-odometry-accuracy",level:3},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"References",id:"references",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-1-navigation-and-localization-overview",children:"Lesson 1: Navigation and Localization Overview"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Duration"}),": 2.5 hours | ",(0,i.jsx)(n.strong,{children:"Level"}),": CEFR B1-C2 | ",(0,i.jsx)(n.strong,{children:"Priority"}),": P1"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explain"})," SLAM algorithms and their role in autonomous navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Distinguish"})," between localization, mapping, and odometry"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Describe"})," the differences between monocular and stereo visual SLAM"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understand"})," sensor requirements for VSLAM systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigate"})," basic ROS 2 SLAM systems and verify odometry output"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Identify"})," loop closure detection and its importance"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"layer-1-foundation",children:"Layer 1: Foundation"}),"\n",(0,i.jsx)(n.h3,{id:"11-what-is-autonomous-navigation",children:"1.1 What is Autonomous Navigation?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Definition"}),": Autonomous navigation is the ability of a robot to move from point A to point B without human control, making intelligent decisions to avoid obstacles and reach its destination."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key components"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization"}),": Knowing where you are"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mapping"}),": Understanding your environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Planning"}),": Deciding how to get somewhere"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control"}),": Executing planned motions"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["In Chapters 1-2, you learned about robots, sensors, and simulation. Now we bring it together: ",(0,i.jsx)(n.strong,{children:"autonomous navigation"})," is the integration of all these components into a system that can move intelligently."]}),"\n",(0,i.jsx)(n.h3,{id:"12-the-three-hard-problems-of-autonomous-navigation",children:"1.2 The Three Hard Problems of Autonomous Navigation"}),"\n",(0,i.jsx)(n.h4,{id:"problem-1-localization-where-am-i",children:"Problem 1: Localization (Where am I?)"}),"\n",(0,i.jsx)(n.p,{children:"Traditional robots use:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPS"}),": Works outdoors, fails indoors/urban canyons"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Odometry"}),": Tracks wheel rotation, but drifts over time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compass"}),": Unreliable due to magnetic interference"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Visual SLAM solution"}),": Use camera images + feature matching to estimate robot position while simultaneously building a map."]}),"\n",(0,i.jsx)(n.h4,{id:"problem-2-mapping-whats-around-me",children:"Problem 2: Mapping (What's around me?)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Raw sensor data (point clouds, images) is high-dimensional and noisy"}),"\n",(0,i.jsx)(n.li,{children:"Must represent obstacles for planning (costmaps)"}),"\n",(0,i.jsx)(n.li,{children:"Must handle dynamic obstacles (moving objects)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Process sensor data into structured maps:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Occupancy grids"}),": Each cell is occupied (1) or free (0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Costmaps"}),': Extend occupancy grids with "cost" for safety margins']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic maps"}),": Include object labels and properties"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"problem-3-planning-how-do-i-get-there",children:"Problem 3: Planning (How do I get there?)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global planning"}),": Find a path from start to goal"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Local planning"}),": Avoid obstacles while executing the path"]}),"\n",(0,i.jsx)(n.li,{children:"Both must be fast (replanning in <2 seconds)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Multi-level planning with different time horizons:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global planner"}),": Runs every 10-20 seconds on full costmap"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Local planner"}),": Runs every 100ms on immediate neighborhood"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"13-slam-the-elegant-solution",children:"1.3 SLAM: The Elegant Solution"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"SLAM"})," = ",(0,i.jsx)(n.strong,{children:"S"}),"imultaneous ",(0,i.jsx)(n.strong,{children:"L"}),"ocalization ",(0,i.jsx)(n.strong,{children:"A"}),"nd ",(0,i.jsx)(n.strong,{children:"M"}),"apping"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The Insight"}),": Instead of solving localization and mapping separately, solve them together!"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Robot moves and observes visual features (corners, edges, textures)"}),"\n",(0,i.jsx)(n.li,{children:"Match features between consecutive images"}),"\n",(0,i.jsx)(n.li,{children:"Estimate robot motion from feature matches"}),"\n",(0,i.jsx)(n.li,{children:"Accumulate features into a global map"}),"\n",(0,i.jsx)(n.li,{children:"When revisiting known areas, use loop closure to correct drift"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why it works"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera provides rich information about the environment"}),"\n",(0,i.jsx)(n.li,{children:"Feature matching is fast and reliable"}),"\n",(0,i.jsx)(n.li,{children:"Loop closure detection prevents unbounded drift"}),"\n",(0,i.jsx)(n.li,{children:"Works without pre-built maps"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Visual SLAM systems"})," use only camera input:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ORB-SLAM3 (our choice): Feature-based, robust, real-time"}),"\n",(0,i.jsx)(n.li,{children:"DSLAM: Direct visual odometry"}),"\n",(0,i.jsx)(n.li,{children:"COLMAP: Offline photogrammetry"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"14-visual-odometry-vs-full-slam",children:"1.4 Visual Odometry vs. Full SLAM"}),"\n",(0,i.jsx)(n.h4,{id:"visual-odometry-vo",children:"Visual Odometry (VO)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Estimates robot motion frame-to-frame"}),"\n",(0,i.jsx)(n.li,{children:"Accumulates drift over long paths"}),"\n",(0,i.jsx)(n.li,{children:"Lightweight, works in real-time"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Equation"}),": position(t) = position(t-1) + delta_motion(t)"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"visual-slam-full-slam",children:"Visual SLAM (full SLAM)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Includes global optimization with loop closure"}),"\n",(0,i.jsx)(n.li,{children:"Corrects drift when revisiting locations"}),"\n",(0,i.jsx)(n.li,{children:"Heavier computation, but more accurate"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Benefit"}),": Long-term accuracy without drift"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-world analogy"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VO"}),": Taking steps and counting, useful for short paths"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SLAM"}),": Remembering landmarks and correcting when you see them again"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"15-monocular-vs-stereo-vslam",children:"1.5 Monocular vs. Stereo VSLAM"}),"\n",(0,i.jsx)(n.h4,{id:"monocular-slam",children:"Monocular SLAM"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Single camera stream"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": Lightweight, single camera on most robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Weakness"}),": No absolute scale (can't tell if object is big and far or small and close)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Use wheel odometry to learn scale"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Pixel difference \u2500\u2192 Feature motion\r\n                    (angle only, not distance)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"stereo-slam",children:"Stereo SLAM"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input"}),": Two synchronized cameras"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": Absolute scale from stereo baseline"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Weakness"}),": Requires stereo calibration, more computation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Benefit"}),": Scale known without odometry"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Left image + Right image \u2500\u2192 Stereo disparity \u2500\u2192 Depth \u2500\u2192 Scale information\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Trade-off"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Monocular: Fast, simple, needs odometry fusion"}),"\n",(0,i.jsx)(n.li,{children:"Stereo: Accurate, complex setup, works standalone"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"layer-2-collaboration-ros-2-integration",children:"Layer 2: Collaboration (ROS 2 Integration)"}),"\n",(0,i.jsx)(n.h3,{id:"21-slam-in-ros-2",children:"2.1 SLAM in ROS 2"}),"\n",(0,i.jsxs)(n.p,{children:["ORB-SLAM3 can run as a ",(0,i.jsx)(n.strong,{children:"ROS 2 node"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Gazebo Simulator                        \u2502\r\n\u2502  \u251c\u2500 /gazebo/image_raw (camera topic)  \u2502\r\n\u2502  \u251c\u2500 /gazebo/odom (wheel odometry)     \u2502\r\n\u2502  \u2514\u2500 /gazebo/tf (ground truth frames)  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n               \u2502 publishes camera images\r\n               \u25bc\r\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n         \u2502 ORB-SLAM3    \u2502 ROS 2 wrapper\r\n         \u2502 ROS 2 Node   \u2502\r\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                \u2502 publishes:\r\n                \u251c\u2500 /orb_slam3/pose (estimated position)\r\n                \u251c\u2500 /orb_slam3/pose_covariance\r\n                \u251c\u2500 /orb_slam3/map (feature map)\r\n                \u2514\u2500 /orb_slam3/tracked_features\r\n               \u2502 subscribes:\r\n               \u2514\u2500 /camera/image_raw (input)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"22-key-topics-and-services",children:"2.2 Key Topics and Services"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Topic"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Direction"}),(0,i.jsx)(n.th,{children:"Content"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/camera/image_raw"})}),(0,i.jsx)(n.td,{children:"sensor_msgs/Image"}),(0,i.jsx)(n.td,{children:"In"}),(0,i.jsx)(n.td,{children:"Camera image from Gazebo"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/odom"})}),(0,i.jsx)(n.td,{children:"nav_msgs/Odometry"}),(0,i.jsx)(n.td,{children:"Out"}),(0,i.jsx)(n.td,{children:"Robot pose and velocity estimates"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/tf"})}),(0,i.jsx)(n.td,{children:"tf2_msgs/TFMessage"}),(0,i.jsx)(n.td,{children:"Out"}),(0,i.jsx)(n.td,{children:"Frame transformations (map \u2192 odom \u2192 base_link)"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"23-coordinate-frames",children:"2.3 Coordinate Frames"}),"\n",(0,i.jsx)(n.p,{children:"SLAM establishes several frames:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"map  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500(T_map_odom)\u2500\u2500\u2500\u2500\u2500\u2500\u25ba odom  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500(T_odom_base)\u2500\u2500\u2500\u2500\u2500\u2500\u25ba base_link\r\n  (global)                         (drift)                              (robot)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"map"}),": Global frame (SLAM output, doesn't drift long-term)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"odom"}),": Odometry frame (drifts over time, wheel encoder origin)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"base_link"}),": Robot center (moves with robot)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Loop closure effect"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"After loop closure detection:\r\nT_map_odom gets updated to correct accumulated drift\n"})}),"\n",(0,i.jsx)(n.h2,{id:"layer-3-intelligence-tuning-and-optimization",children:"Layer 3: Intelligence (Tuning and Optimization)"}),"\n",(0,i.jsx)(n.h3,{id:"31-loop-closure-detection",children:"3.1 Loop Closure Detection"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Over long paths, odometry drifts. How do we know when we've returned to a known location?"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Feature matching and place recognition."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Store visual features and their 3D positions"}),"\n",(0,i.jsx)(n.li,{children:"When new features appear, compare to stored features"}),"\n",(0,i.jsx)(n.li,{children:"If match found with features from earlier position \u2192 loop closure!"}),"\n",(0,i.jsx)(n.li,{children:"Optimize global pose graph to correct drift"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Importance"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Without loop closure: drift unbounded"}),"\n",(0,i.jsx)(n.li,{children:"With loop closure: long-term accuracy maintained"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-world example"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robot explores a warehouse floor plan (200m path)"}),"\n",(0,i.jsx)(n.li,{children:"After 200m, robot returns to start location"}),"\n",(0,i.jsx)(n.li,{children:"Camera sees the starting room again"}),"\n",(0,i.jsx)(n.li,{children:"Features matched with stored features from time t=0"}),"\n",(0,i.jsx)(n.li,{children:"SLAM system corrects accumulated drift"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"32-tuning-slam-parameters",children:"3.2 Tuning SLAM Parameters"}),"\n",(0,i.jsx)(n.p,{children:"Key parameters for ORB-SLAM3:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Parameter"}),(0,i.jsx)(n.th,{children:"Effect"}),(0,i.jsx)(n.th,{children:"Range"}),(0,i.jsx)(n.th,{children:"Typical"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"feature_threshold"})}),(0,i.jsx)(n.td,{children:"Number of features detected"}),(0,i.jsx)(n.td,{children:"0.01-0.1"}),(0,i.jsx)(n.td,{children:"0.05"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"min_matches"})}),(0,i.jsx)(n.td,{children:"Minimum matches between frames"}),(0,i.jsx)(n.td,{children:"5-50"}),(0,i.jsx)(n.td,{children:"20"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"scale_factor"})}),(0,i.jsx)(n.td,{children:"Image pyramid scale"}),(0,i.jsx)(n.td,{children:"1.1-1.3"}),(0,i.jsx)(n.td,{children:"1.2"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"num_levels"})}),(0,i.jsx)(n.td,{children:"Pyramid levels"}),(0,i.jsx)(n.td,{children:"1-8"}),(0,i.jsx)(n.td,{children:"4"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tuning strategy"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Start with defaults"}),"\n",(0,i.jsxs)(n.li,{children:["If low odometry accuracy \u2192 increase ",(0,i.jsx)(n.code,{children:"min_matches"})]}),"\n",(0,i.jsxs)(n.li,{children:["If features missed \u2192 decrease ",(0,i.jsx)(n.code,{children:"feature_threshold"})]}),"\n",(0,i.jsxs)(n.li,{children:["If computational expensive \u2192 reduce ",(0,i.jsx)(n.code,{children:"num_levels"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"33-accuracy-metrics",children:"3.3 Accuracy Metrics"}),"\n",(0,i.jsx)(n.p,{children:"How do we measure SLAM performance?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Absolute Trajectory Error (ATE)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ATE = sqrt(mean((estimated_pose - ground_truth_pose)\xb2))\n"})}),"\n",(0,i.jsx)(n.p,{children:"Measures overall localization accuracy. Target: <5% of path length."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Relative Pose Error (RPE)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"RPE = measured between poses at t and t+\u0394t\n"})}),"\n",(0,i.jsx)(n.p,{children:"Measures local odometry quality. Target: <2% of distance traveled."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Loop Closure Precision"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"% of correctly detected loop closures\n"})}),"\n",(0,i.jsx)(n.p,{children:"Target: >95% true positives, <5% false positives."}),"\n",(0,i.jsx)(n.h2,{id:"layer-4-advanced-research-frontiers",children:"Layer 4: Advanced (Research Frontiers)"}),"\n",(0,i.jsx)(n.h3,{id:"41-beyond-orb-slam3",children:"4.1 Beyond ORB-SLAM3"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Deep Learning SLAM"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Neural networks to replace hand-crafted features"}),"\n",(0,i.jsx)(n.li,{children:"Advantages: Works in challenging conditions (low light, motion blur)"}),"\n",(0,i.jsx)(n.li,{children:"Disadvantages: Requires large training datasets"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LiDAR-based SLAM"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Uses laser range finder instead of camera"}),"\n",(0,i.jsx)(n.li,{children:"Advantages: Range information, works in darkness"}),"\n",(0,i.jsx)(n.li,{children:"Disadvantages: Sparse, requires rotation for full range"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multi-sensor SLAM"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Combines camera + LiDAR + IMU"}),"\n",(0,i.jsx)(n.li,{children:"Better robustness and accuracy"}),"\n",(0,i.jsx)(n.li,{children:"More complex state management"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"42-loop-closure-in-real-world-systems",children:"4.2 Loop Closure in Real-World Systems"}),"\n",(0,i.jsx)(n.p,{children:"Real challenges:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Appearance change"}),": Same location looks different (time of day, weather, dynamic objects)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perceptual aliasing"}),": Different locations look similar"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Large rotations"}),": Detecting loops after 180\xb0 turn difficult"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Modern solutions:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Place recognition networks (e.g., NetVLAD)"}),"\n",(0,i.jsx)(n.li,{children:"Geometric consistency checks"}),"\n",(0,i.jsx)(n.li,{children:"Temporal filtering to reject transient loop closure detections"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Concept"}),(0,i.jsx)(n.th,{children:"Key Insight"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Autonomous Navigation"})}),(0,i.jsx)(n.td,{children:"Integration of localization, mapping, planning, and control"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"SLAM"})}),(0,i.jsx)(n.td,{children:"Simultaneous localization and mapping solves drift problem"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Visual Odometry"})}),(0,i.jsx)(n.td,{children:"Monocular frame-to-frame motion, accumulates drift"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Visual SLAM"})}),(0,i.jsx)(n.td,{children:"Adds loop closure for long-term accuracy"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Monocular"})}),(0,i.jsx)(n.td,{children:"Fast, needs scale calibration"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Stereo"})}),(0,i.jsx)(n.td,{children:"Accurate, requires synchronization and calibration"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Loop Closure"})}),(0,i.jsx)(n.td,{children:"Critical for long-term accuracy in large environments"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,i.jsx)(n.p,{children:"Check your understanding:"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Can you explain what SLAM stands for and what it solves?"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Do you understand the difference between localization and odometry?"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Can you explain monocular vs. stereo trade-offs?"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Do you know why loop closure detection is important?"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Can you describe the map \u2192 odom \u2192 base_link frame hierarchy?"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Full assessment"}),": See ",(0,i.jsx)(n.a,{href:"../checklists/slam-comprehension.md",children:"SLAM Comprehension Checklist"})]}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.h3,{id:"example-1-launch-orb-slam3-on-gazebo",children:"Example 1: Launch ORB-SLAM3 on Gazebo"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# code_examples/orb_slam3_launch.py\r\nimport launch\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return launch.LaunchDescription([\r\n        # Gazebo simulator\r\n        Node(\r\n            package='gazebo_ros',\r\n            executable='gazebo',\r\n            arguments=['../../gazebo_worlds/flat_navigation_world.world'],\r\n        ),\r\n        # ORB-SLAM3 ROS 2 wrapper\r\n        Node(\r\n            package='orb_slam3_ros2',\r\n            executable='mono_inertial_node',\r\n            parameters=[{\r\n                'image_topic': '/camera/image_raw',\r\n                'vocab_file': '/path/to/ORBvoc.txt',\r\n                'settings_file': '/path/to/settings.yaml',\r\n            }],\r\n        ),\r\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-2-verify-odometry-accuracy",children:"Example 2: Verify Odometry Accuracy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# code_examples/slam_accuracy_checker.py\r\nimport rclpy\r\nfrom nav_msgs.msg import Odometry\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport numpy as np\r\n\r\nclass SLAMValidator(rclpy.Node):\r\n    def __init__(self):\r\n        super().__init__('slam_validator')\r\n        self.slam_poses = []\r\n        self.ground_truth_poses = []\r\n\r\n        # Subscribe to SLAM output\r\n        self.create_subscription(\r\n            Odometry, '/orb_slam3/pose', self.slam_callback, 10)\r\n\r\n        # Subscribe to ground truth from Gazebo\r\n        self.create_subscription(\r\n            PoseStamped, '/gazebo/model_states', self.gt_callback, 10)\r\n\r\n    def slam_callback(self, msg):\r\n        pose = msg.pose.pose\r\n        self.slam_poses.append([pose.position.x, pose.position.y])\r\n        self.compute_ate()\r\n\r\n    def compute_ate(self):\r\n        # Absolute Trajectory Error\r\n        if len(self.slam_poses) > 10:\r\n            ate = np.sqrt(np.mean(\r\n                (np.array(self.slam_poses) -\r\n                 np.array(self.ground_truth_poses))**2\r\n            ))\r\n            self.get_logger().info(f'ATE: {ate:.4f}m')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Run SLAM on Gazebo and verify odometry output"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Launch the provided Gazebo world: ",(0,i.jsx)(n.code,{children:"gazebo gazebo_worlds/flat_navigation_world.world"})]}),"\n",(0,i.jsxs)(n.li,{children:["Start ORB-SLAM3: ",(0,i.jsx)(n.code,{children:"ros2 launch code_examples/orb_slam3_launch.py"})]}),"\n",(0,i.jsxs)(n.li,{children:["Record the odometry: ",(0,i.jsx)(n.code,{children:"ros2 bag record /orb_slam3/pose"})]}),"\n",(0,i.jsxs)(n.li,{children:["Move robot through environment using ",(0,i.jsx)(n.code,{children:"/gazebo/cmd_vel"})," publisher"]}),"\n",(0,i.jsxs)(n.li,{children:["After 5 minutes, analyze odometry accuracy using ",(0,i.jsx)(n.code,{children:"slam_accuracy_checker.py"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Success criterion"}),": ATE < 5% of total path length"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ORB-SLAM3"}),": ",(0,i.jsx)(n.a,{href:"https://github.com/UZ-SLAMLab/ORB_SLAM3",children:"GitHub Repository"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual SLAM Survey"}),": ",(0,i.jsx)(n.a,{href:"https://www.youtube.com/playlist?list=PLgnQpQmwLSnxIQmHKzIUM2Ijy6J3-Qt5T",children:"Introduction to Visual SLAM (Cyrill Stachniss)"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 SLAM Setup"}),": ",(0,i.jsx)(n.a,{href:"https://github.com/SteveMacenski/slam_toolbox",children:"Nav2 SLAM Toolbox"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Now that you understand SLAM fundamentals and localization:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced option"}),": Proceed to ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/visual-slam-systems",children:"Lesson 2: Visual SLAM Systems"})," for deeper technical details on ORB-SLAM3"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Core path"}),": Jump to ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/nav2-path-planning-stack",children:"Lesson 4: Nav2 Path Planning"})," to learn how to plan paths using SLAM for localization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optional"}),": Explore ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/introduction-to-isaac-sim",children:"Lesson 3: Isaac Sim"})," for photorealistic simulation alternatives"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Lesson 1 Summary"}),": You now understand SLAM as the solution to autonomous navigation's fundamental challenge: simultaneously localizing and mapping. You know the differences between visual odometry and full SLAM, understand monocular vs. stereo trade-offs, and recognize loop closure detection's critical importance."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ready for the next lesson?"})," Choose your path:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["\u2192 ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/visual-slam-systems",children:"Lesson 2: Visual SLAM Systems"})]})," (detailed SLAM algorithms)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["\u2192 ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/nav2-path-planning-stack",children:"Lesson 4: Nav2 Path Planning"})]})," (core curriculum)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["\u2192 ",(0,i.jsx)(n.a,{href:"/docs/Autonomous-Navigation/introduction-to-isaac-sim",children:"Lesson 3: Isaac Sim"})]})," (advanced simulation)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>t});var r=s(6540);const i={},o=r.createContext(i);function l(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);