"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[269],{3912:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Autonomous-Navigation/multi-sensor-perception-and-fusion","title":"Lesson 7: Multi-Sensor Perception and Fusion","description":"Duration Unlimited | Priority Lesson 1 or 3","source":"@site/docs/04-Autonomous-Navigation/07-multi-sensor-perception-and-fusion.md","sourceDirName":"04-Autonomous-Navigation","slug":"/Autonomous-Navigation/multi-sensor-perception-and-fusion","permalink":"/docs/Autonomous-Navigation/multi-sensor-perception-and-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/ai-book/tree/main/docs/04-Autonomous-Navigation/07-multi-sensor-perception-and-fusion.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6: Autonomous Humanoid Navigation","permalink":"/docs/Autonomous-Navigation/autonomous-humanoid-navigation"},"next":{"title":"Lesson 8: Capstone - Autonomous Navigation End-to-End","permalink":"/docs/Autonomous-Navigation/capstone-mission"}}');var s=r(4848),o=r(8453);const t={},a="Lesson 7: Multi-Sensor Perception and Fusion",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Layer 1: Foundation",id:"layer-1-foundation",level:2},{value:"7.1 Sensor Characteristics",id:"71-sensor-characteristics",level:3},{value:"Camera (Visual)",id:"camera-visual",level:4},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:4},{value:"IMU (Accelerometer + Gyroscope)",id:"imu-accelerometer--gyroscope",level:4},{value:"7.2 Sensor Fusion Motivation",id:"72-sensor-fusion-motivation",level:3},{value:"7.3 Fusion Architectures",id:"73-fusion-architectures",level:3},{value:"7.4 Covariance-Weighted Fusion",id:"74-covariance-weighted-fusion",level:3},{value:"Layer 2: Collaboration (ROS 2 Integration)",id:"layer-2-collaboration-ros-2-integration",level:2},{value:"2.1 Temporal Synchronization",id:"21-temporal-synchronization",level:3},{value:"2.2 Using message_filters for Synchronization",id:"22-using-message_filters-for-synchronization",level:3},{value:"2.3 Sensor Fusion Node Architecture",id:"23-sensor-fusion-node-architecture",level:3},{value:"Layer 3: Intelligence (Fusion Tuning)",id:"layer-3-intelligence-fusion-tuning",level:2},{value:"3.1 Covariance Tuning",id:"31-covariance-tuning",level:3},{value:"3.2 Failure Mode Handling",id:"32-failure-mode-handling",level:3},{value:"3.3 Measuring Fusion Performance",id:"33-measuring-fusion-performance",level:3},{value:"Layer 4: Advanced",id:"layer-4-advanced",level:2},{value:"4.1 Extended Kalman Filter (EKF) Fusion",id:"41-extended-kalman-filter-ekf-fusion",level:3},{value:"Summary",id:"summary",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Example 1: Multi-Sensor Fusion Node",id:"example-1-multi-sensor-fusion-node",level:3},{value:"Example 2: Sensor Validation",id:"example-2-sensor-validation",level:3},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"References",id:"references",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-7-multi-sensor-perception-and-fusion",children:"Lesson 7: Multi-Sensor Perception and Fusion"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Duration"}),": 3 hours | ",(0,s.jsx)(n.strong,{children:"Level"}),": Unlimited | ",(0,s.jsx)(n.strong,{children:"Priority"}),": P3 | ",(0,s.jsx)(n.strong,{children:"Prerequisite"}),": Lesson 1 or 3"]}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explain"})," sensor characteristics (camera, LiDAR, IMU) and their trade-offs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," temporal synchronization using ROS 2 message_filters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design"})," covariance-weighted sensor fusion strategies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Measure"})," accuracy improvements from multi-sensor fusion (>20% target)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Handle"})," sensor failures and gracefully degrade"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"layer-1-foundation",children:"Layer 1: Foundation"}),"\n",(0,s.jsx)(n.h3,{id:"71-sensor-characteristics",children:"7.1 Sensor Characteristics"}),"\n",(0,s.jsx)(n.h4,{id:"camera-visual",children:"Camera (Visual)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Strengths:\r\n\u251c\u2500 Rich visual information (features, colors, textures)\r\n\u251c\u2500 Works well in daylight/indoor with good lighting\r\n\u2514\u2500 Scalable (multiple cameras cheap)\r\n\r\nWeaknesses:\r\n\u251c\u2500 Fails in darkness or low contrast\r\n\u251c\u2500 Motion blur at high speed\r\n\u251c\u2500 Computationally expensive feature extraction\r\n\u2514\u2500 Sensitive to lighting changes\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Good for"}),": Feature-based SLAM, object detection"]}),"\n",(0,s.jsx)(n.h4,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Strengths:\r\n\u251c\u2500 Robust distance measurement (works day/night)\r\n\u251c\u2500 3D point cloud, good range (5-30m)\r\n\u251c\u2500 Less affected by lighting\r\n\u2514\u2500 Direct depth, no scale ambiguity\r\n\r\nWeaknesses:\r\n\u251c\u2500 Sparse point clouds (vs. dense images)\r\n\u251c\u2500 Expensive hardware\r\n\u251c\u2500 Doesn't capture color/texture\r\n\u2514\u2500 Limited by range and reflectivity\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Good for"}),": Range sensing, obstacle detection"]}),"\n",(0,s.jsx)(n.h4,{id:"imu-accelerometer--gyroscope",children:"IMU (Accelerometer + Gyroscope)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Strengths:\r\n\u251c\u2500 Immediate motion feedback (no latency)\r\n\u251c\u2500 Works anywhere (indoors/outdoors, dark/light)\r\n\u251c\u2500 Small, cheap, lightweight\r\n\u2514\u2500 No external dependencies\r\n\r\nWeaknesses:\r\n\u251c\u2500 Accelerometer drifts (integration error accumulates)\r\n\u251c\u2500 Gyroscope drifts (bias accumulates)\r\n\u251c\u2500 Noisy measurements\r\n\u2514\u2500 Can't estimate absolute position alone\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Good for"}),": Motion tracking, orientation"]}),"\n",(0,s.jsx)(n.h3,{id:"72-sensor-fusion-motivation",children:"7.2 Sensor Fusion Motivation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Single sensor limitations"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera in dark room:\r\n\u251c\u2500 Can't see features \u2192 SLAM fails\r\n\u2514\u2500 Output: No pose estimate\r\n\r\nLiDAR in featureless corridor:\r\n\u251c\u2500 All similar point clouds \u2192 ambiguous\r\n\u2514\u2500 Output: Multiple possible poses\r\n\r\nIMU alone:\r\n\u251c\u2500 Drifts over time\r\n\u2514\u2500 Output: Rough estimate, diverges\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fused sensors"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera + LiDAR + IMU:\r\n\u251c\u2500 Camera: rich features (when light available)\r\n\u251c\u2500 LiDAR: robust range (works day/night)\r\n\u251c\u2500 IMU: immediate motion feedback\r\n\u2514\u2500 Output: Robust pose with 20%+ accuracy improvement\n"})}),"\n",(0,s.jsx)(n.h3,{id:"73-fusion-architectures",children:"7.3 Fusion Architectures"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tightly-coupled fusion"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"All sensors feed into single optimizer"}),"\n",(0,s.jsx)(n.li,{children:"Best accuracy, most complex"}),"\n",(0,s.jsx)(n.li,{children:"Example: Visual-inertial SLAM"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Loosely-coupled fusion"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each sensor produces independent pose estimate"}),"\n",(0,s.jsx)(n.li,{children:"Estimates combined via weighted average"}),"\n",(0,s.jsx)(n.li,{children:"Simpler, sufficient for most applications"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Our approach"}),": Loosely-coupled with covariance weighting"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Visual odometry \u2192 pose (with covariance)\r\n                      \u2502\r\n                      \u251c\u2192 Weighted fusion \u2192 final pose\r\n                      \u2502\r\nLiDAR odometry \u2192 pose (with covariance)\r\n                      \u2502\r\nIMU-based estimate \u2192 orientation (with covariance)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"74-covariance-weighted-fusion",children:"7.4 Covariance-Weighted Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Each sensor produces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State estimate"}),": Position and orientation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Covariance"}),": Uncertainty matrix (how confident?)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fusion rule"})," (Kalman filter-like):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Measurement 1: pose_1, cov_1 (high confidence, low covariance)\r\nMeasurement 2: pose_2, cov_2 (low confidence, high covariance)\r\n\r\nFused estimate = (cov_2*pose_1 + cov_1*pose_2) / (cov_1 + cov_2)\r\n\r\nIntuition: Weight measurements by inverse covariance\n"})}),"\n",(0,s.jsx)(n.h2,{id:"layer-2-collaboration-ros-2-integration",children:"Layer 2: Collaboration (ROS 2 Integration)"}),"\n",(0,s.jsx)(n.h3,{id:"21-temporal-synchronization",children:"2.1 Temporal Synchronization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Challenge"}),": Sensors have different frequencies and latencies"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera @ 30 Hz:    [    @0.1s   @0.2s   @0.3s   ]\r\nLiDAR @ 10 Hz:     [   @0.0s            @0.2s  ]\r\nIMU @ 100 Hz:      [@ @ @ @ @ @ @ @ @ @ @ @ @ @ ]\r\n\r\nHow do we combine at same time?\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": ",(0,s.jsx)(n.code,{children:"message_filters"})," with exact/approximate time synchronization"]}),"\n",(0,s.jsx)(n.h3,{id:"22-using-message_filters-for-synchronization",children:"2.2 Using message_filters for Synchronization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# code_examples/temporal_sync_handler.py\r\nimport rclpy\r\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom sensor_msgs.msg import Image, Imu\r\n\r\nclass SensorFusionNode(rclpy.Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_fusion')\r\n\r\n        # Create subscribers\r\n        visual_sub = Subscriber(self, PoseStamped, '/camera/odometry')\r\n        lidar_sub = Subscriber(self, PoseStamped, '/lidar/odometry')\r\n        imu_sub = Subscriber(self, Imu, '/imu/data')\r\n\r\n        # Synchronize messages\r\n        # ApproximateTimeSynchronizer allows time slew (0.1s tolerance)\r\n        sync = ApproximateTimeSynchronizer(\r\n            [visual_sub, lidar_sub, imu_sub],\r\n            queue_size=10,\r\n            slop=0.1  # 100ms tolerance\r\n        )\r\n\r\n        sync.registerCallback(self.fusion_callback)\r\n\r\n    def fusion_callback(self, visual, lidar, imu):\r\n        \"\"\"\r\n        Called when all three messages are available\r\n        within 100ms of each other\r\n        \"\"\"\r\n        self.fuse_measurements(visual, lidar, imu)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"23-sensor-fusion-node-architecture",children:"2.3 Sensor Fusion Node Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# sensor_fusion_node_config.yaml\r\nsensor_fusion:\r\n  input_topics:\r\n    visual_odometry: "/camera/odometry"  # from SLAM\r\n    lidar_odometry: "/lidar/odometry"    # from LiDAR-SLAM\r\n    imu_data: "/imu/data"               # raw IMU\r\n\r\n  output_topics:\r\n    fused_pose: "/fused/odometry"       # fused estimate\r\n    diagnostics: "/fused/diagnostics"\r\n\r\n  fusion_parameters:\r\n    use_visual: true                     # enable camera odometry\r\n    use_lidar: true                      # enable LiDAR odometry\r\n    use_imu: true                        # enable IMU\r\n\r\n    sync_tolerance: 0.1  # seconds (100ms)\r\n\r\n    # Weighting (lower covariance = higher weight)\r\n    visual_weight: 1.0   # reference weight\r\n    lidar_weight: 1.5    # LiDAR slightly less trusted\r\n    imu_weight: 2.0      # IMU least trusted for position\r\n\r\n  failure_modes:\r\n    visual_timeout: 2.0    # seconds until visual considered dead\r\n    lidar_timeout: 2.0\r\n    imu_timeout: 1.0\r\n    fallback_sensor: "lidar"  # use this if others fail\n'})}),"\n",(0,s.jsx)(n.h2,{id:"layer-3-intelligence-fusion-tuning",children:"Layer 3: Intelligence (Fusion Tuning)"}),"\n",(0,s.jsx)(n.h3,{id:"31-covariance-tuning",children:"3.1 Covariance Tuning"}),"\n",(0,s.jsx)(n.p,{children:"Each sensor publishes odometry with covariance matrix:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def compute_covariance(sensor_noise, measurement_error):\r\n    """\r\n    Covariance = measure of uncertainty\r\n\r\n    Low covariance (0.01) = high confidence\r\n    High covariance (1.0) = low confidence\r\n    """\r\n    return np.eye(3) * measurement_error\r\n\r\n# Visual odometry covariance\r\n# Usually low in well-lit scenes, high in darkness\r\nvisual_cov = 0.05  # confident in daylight\r\n\r\n# LiDAR odometry covariance\r\n# Usually low, except in featureless corridors\r\nlidar_cov = 0.10   # medium confidence\r\n\r\n# IMU covariance\r\n# High initially, increases with time (drift)\r\nimu_cov_initial = 0.5  # low confidence\r\nimu_cov_over_time = imu_cov_initial + drift_rate * time\n'})}),"\n",(0,s.jsx)(n.h3,{id:"32-failure-mode-handling",children:"3.2 Failure Mode Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# code_examples/fusion_failure_handling.py\r\nclass FusionWithFailover(rclpy.Node):\r\n    def __init__(self):\r\n        self.active_sensors = {'visual': True, 'lidar': True, 'imu': True}\r\n        self.last_message_time = {\r\n            'visual': None, 'lidar': None, 'imu': None\r\n        }\r\n\r\n    def check_sensor_health(self):\r\n        \"\"\"Check which sensors are producing data\"\"\"\r\n        current_time = self.get_clock().now()\r\n\r\n        for sensor in self.active_sensors.keys():\r\n            if self.last_message_time[sensor] is None:\r\n                continue\r\n\r\n            time_since_message = (current_time -\r\n                               self.last_message_time[sensor]).nanoseconds / 1e9\r\n\r\n            if time_since_message > self.timeout[sensor]:\r\n                self.get_logger().warn(f'{sensor} timeout - disabling')\r\n                self.active_sensors[sensor] = False\r\n            else:\r\n                self.active_sensors[sensor] = True\r\n\r\n    def select_fallback(self):\r\n        \"\"\"Use available sensor if others fail\"\"\"\r\n        if self.active_sensors['visual']:\r\n            return 'visual'\r\n        elif self.active_sensors['lidar']:\r\n            return 'lidar'\r\n        elif self.active_sensors['imu']:\r\n            return 'imu'\r\n        else:\r\n            return None  # all sensors failed!\n"})}),"\n",(0,s.jsx)(n.h3,{id:"33-measuring-fusion-performance",children:"3.3 Measuring Fusion Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ground truth comparison"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Compare fused pose to ground truth\r\nerror_before = compute_ate(visual_only, ground_truth)\r\nerror_after = compute_ate(fused_pose, ground_truth)\r\n\r\nimprovement = (error_before - error_after) / error_before * 100\r\n# Target: >20% improvement\n"})}),"\n",(0,s.jsx)(n.h2,{id:"layer-4-advanced",children:"Layer 4: Advanced"}),"\n",(0,s.jsx)(n.h3,{id:"41-extended-kalman-filter-ekf-fusion",children:"4.1 Extended Kalman Filter (EKF) Fusion"}),"\n",(0,s.jsx)(n.p,{children:"More advanced than simple weighted averaging:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"EKF maintains:\r\n\u251c\u2500 State: [x, y, theta, vx, vy, vtheta]\r\n\u251c\u2500 Covariance: Uncertainty in each state variable\r\n\u2514\u2500 Update cycle:\r\n    1. Predict: Use motion model\r\n    2. Measure: Receive sensor measurements\r\n    3. Correct: Weight update by sensor covariance\r\n    4. Repeat\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Concept"}),(0,s.jsx)(n.th,{children:"Key Insight"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Sensor fusion"})}),(0,s.jsx)(n.td,{children:"Combines strengths, mitigates weaknesses"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Covariance"})}),(0,s.jsx)(n.td,{children:"Measures uncertainty; inverse of confidence"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Synchronization"})}),(0,s.jsx)(n.td,{children:"Temporal alignment critical for fusion"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Loosely-coupled"})}),(0,s.jsx)(n.td,{children:"Simple, sufficient for most applications"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Failure handling"})}),(0,s.jsx)(n.td,{children:"Monitor sensor health, fallback gracefully"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-1-multi-sensor-fusion-node",children:"Example 1: Multi-Sensor Fusion Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# code_examples/sensor_fusion_node.py\r\nimport numpy as np\r\nimport rclpy\r\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom sensor_msgs.msg import Imu\r\nfrom nav_msgs.msg import Odometry\r\n\r\nclass MultiSensorFusion(rclpy.Node):\r\n    def __init__(self):\r\n        super().__init__('multi_sensor_fusion')\r\n\r\n        # Subscribers with synchronization\r\n        visual_sub = Subscriber(self, PoseStamped, '/camera/odometry')\r\n        lidar_sub = Subscriber(self, PoseStamped, '/lidar/odometry')\r\n        imu_sub = Subscriber(self, Imu, '/imu/data')\r\n\r\n        sync = ApproximateTimeSynchronizer(\r\n            [visual_sub, lidar_sub, imu_sub],\r\n            queue_size=10, slop=0.1\r\n        )\r\n        sync.registerCallback(self.fusion_callback)\r\n\r\n        self.fused_pub = self.create_publisher(\r\n            Odometry, '/fused/odometry', 10)\r\n\r\n    def fusion_callback(self, visual, lidar, imu):\r\n        \"\"\"Fuse three sensor modalities\"\"\"\r\n\r\n        # Extract covariances\r\n        visual_cov = 0.05  # high confidence\r\n        lidar_cov = 0.10   # medium confidence\r\n        imu_cov = 0.5      # low confidence for position\r\n\r\n        # Weight by inverse covariance\r\n        weights = np.array([1/visual_cov, 1/lidar_cov, 1/imu_cov])\r\n        weights /= weights.sum()  # normalize\r\n\r\n        # Weighted average of positions\r\n        positions = np.array([\r\n            [visual.pose.position.x, visual.pose.position.y],\r\n            [lidar.pose.position.x, lidar.pose.position.y],\r\n            [imu.orientation.x, imu.orientation.y]  # IMU limited\r\n        ])\r\n\r\n        fused_position = weights.reshape(-1, 1) * positions\r\n        fused_position = fused_position.sum(axis=0)\r\n\r\n        # Publish fused estimate\r\n        odometry = Odometry()\r\n        odometry.pose.pose.position.x = fused_position[0]\r\n        odometry.pose.pose.position.y = fused_position[1]\r\n        odometry.header.stamp = self.get_clock().now().to_msg()\r\n        odometry.header.frame_id = 'map'\r\n\r\n        self.fused_pub.publish(odometry)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"example-2-sensor-validation",children:"Example 2: Sensor Validation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# code_examples/validate_sensor_fusion_accuracy.py\r\nimport numpy as np\r\n\r\nclass FusionValidator(rclpy.Node):\r\n    def __init__(self):\r\n        super().__init__('fusion_validator')\r\n        self.single_sensor_errors = []\r\n        self.fused_errors = []\r\n\r\n    def evaluate_accuracy(self, single_sensor, fused, ground_truth):\r\n        \"\"\"\r\n        Measure improvement from fusion\r\n        \"\"\"\r\n        single_error = np.linalg.norm(single_sensor - ground_truth)\r\n        fused_error = np.linalg.norm(fused - ground_truth)\r\n\r\n        improvement = (single_error - fused_error) / single_error * 100\r\n\r\n        self.get_logger().info(\r\n            f'Single sensor error: {single_error:.3f}m\\n'\r\n            f'Fused error: {fused_error:.3f}m\\n'\r\n            f'Improvement: {improvement:.1f}%'\r\n        )\r\n\r\n        return improvement\n"})}),"\n",(0,s.jsx)(n.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Implement multi-sensor fusion and verify >20% accuracy improvement"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Set up three odometry sources:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Visual odometry (camera-based SLAM)"}),"\n",(0,s.jsx)(n.li,{children:"LiDAR odometry (if available, or use simulated)"}),"\n",(0,s.jsx)(n.li,{children:"IMU (gyroscope-based orientation)"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Implement temporal synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Create fusion node with weighted averaging"}),"\n",(0,s.jsxs)(n.li,{children:["Run test scenario in Gazebo:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Well-lit room (cameras work well)"}),"\n",(0,s.jsx)(n.li,{children:"Dark room (cameras fail, LiDAR works)"}),"\n",(0,s.jsx)(n.li,{children:"Feature-poor corridor (LiDAR ambiguous)"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Measure accuracy improvement"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Success criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fusion node runs without errors"}),"\n",(0,s.jsx)(n.li,{children:"Temporal synchronization working (topics synchronized)"}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"20% accuracy improvement over best single sensor"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Graceful failure handling (fallback when sensor dies)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion Survey"}),": ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1902.01305",children:"Multi-Sensor Fusion Review"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extended Kalman Filter"}),": ",(0,s.jsx)(n.a,{href:"https://www.kalmanfilter.net/multivariate_k.html",children:"EKF Tutorial"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 message_filters"}),": ",(0,s.jsx)(n.a,{href:"http://wiki.ros.org/message_filters",children:"Message Filters Documentation"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"You now understand multi-sensor fusion for robust perception. Choose your path:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["\u2192 ",(0,s.jsx)(n.a,{href:"/docs/Autonomous-Navigation/capstone-mission",children:"Lesson 8: Capstone"})]})," (Integrate complete system)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Lesson 7 Summary"}),": Multi-sensor fusion combines visual, range, and inertial information for >20% accuracy improvement. Temporal synchronization, covariance weighting, and failure handling enable robust autonomous navigation in diverse conditions."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>a});var i=r(6540);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);